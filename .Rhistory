legend
matplot(1:13,cbind(train_error,test_error), pch=19, col=c("red","blue"), type = "b",
xlab = "Cant var Predictoras", ylab="Cuadrado Medio esperado")
par(xpd=TRUE)
legend(6,21, legend=c("Error de Entrenamiento","Error de Validación"),pch=19, col=c("red","blue"), horiz = F)
legend(6,15, legend=c("Error de Entrenamiento","Error de Validación"),pch=19, col=c("red","blue"), horiz = F)
legend("rightcenter", legend=c("Error de Entrenamiento","Error de Validación"),pch=19, col=c("red","blue"), horiz = F)
matplot(1:13,cbind(train_error,test_error),
pch=19,
col=c("red","blue"),
type = "b",
xlab = "Cant var Predictoras", ylab="Cuadrado Medio esperado")
par(xpd=TRUE)
legend("right",
legend=c("Error de Entrenamiento","Error de Validación"),
pch=19,
col=c("red","blue"),
horiz = F)
?par
# Graficamos
par(mfrow=c(1,1))
matplot(1:13,cbind(train_error,test_error),
pch=19,
col=c("red","blue"),
type = "b",
xlab = "Cant var Predictoras", ylab="Cuadrado Medio esperado")
legend("right",
legend=c("Error de Entrenamiento","Error de Validación"),
pch=19,
col=c("red","blue"),
horiz = F)
library(randomForest)
ds = read.csv("C:/Users/Battaglia/Desktop/Cursos/DataMining FIUBA/Clases/06 - Árboles de Decisión/Churn_Modelling.csv")
# quito campos que no uso  factorizo la variabnle objetivo
ds = ds[-c(1:3)]
ds$Exited = factor(ds$Exited, levels = c(0, 1))
str(ds)
set.seed(1)
train = sample(nrow(ds), 0.8*nrow(ds))
set.seed(1)
rf=randomForest(Exited~., data=ds , subset=train , mtry=3 , importance=TRUE , ntree=1000)
importance(rf)
rf
# probemos con diferebntes tamaño de árboles
cant.arb =seq(100, 1000, by=100 )
cant.arb
acc.vec <- double(10)
for (i in 1:length(cant.arb)) {
rf=randomForest(Exited~., data=ds , subset=train , mtry=3 , importance=TRUE,  ntree=cant.arb[i])
pred.rf.prob = predict(rf,newdata=ds[-train,], type="prob", ntree=cant.arb[i])
umbral=0.5
pred.umbral <- ifelse(pred.rf.prob[,2] > umbral, 1, 0)
acc.vec[i] = sum(diag(table(pred.umbral,ds[-train,"Exited"])))/nrow(ds[-train,])
}
plot(cant.arb,acc.vec)
#  ahora evaluando los  errores de clasificación al variar la cantidad de variables prdictoras
cant.mtry =seq(1, 10)
#  ahora evaluando los  errores de clasificación al variar la cantidad de variables prdictoras
cant.mtry =seq(1, 10)
error.vec <- double(length(cant.mtry))
cant.mtry =seq(1, 10)
error.vec <- double(length(cant.mtry))
cant.mtry
error.vec
for (i in cant.mtry) {
rf=randomForest(Exited~.,data=ds,subset=train,mtry=i,importance=TRUE, ntree=300)
pred.rf.prob = predict(rf,newdata=ds[-train,], type="prob", ntree=300)
umbral=0.5
pred.umbral <- ifelse(pred.rf.prob[,2] > umbral, 1, 0)
error.vec[i] = 1-sum(diag(table(pred.umbral,ds[-train,"Exited"])))/nrow(ds[-train,])
}
matplot(cant.mtry,error.vec, pch=19, col=c("red","blue"), type = "b",
xlab = "Cant Var Predictoras", ylab="Accuracy")
mtry_min=which.min(error.vec)
matplot(cant.mtry,error.vec, pch=19, col=c("red","blue"), type = "b",
xlab = "Cant Var Predictoras", ylab="Error")
mtry_min=which.min(error.vec)
mtry_min
mtry_min
rf_optimo=randomForest(Exited~.,data=ds,subset=train,mtry=mtry_min,importance=TRUE, ntree=300)
# ordenada por MeanDecreaseAccuracy
importance(rf)[rev(order(importance(rf_optimo)[,3])),]
# ordenada por MeanDecreaseGini
importance(rf)[rev(order(importance(rf_optimo)[,4])),]
# Lo grafico
varImpPlot(rf_optimo)
rf_optimo
#  GBM = GRADIENT BOOSTED MACHINES
#install.packages("gbm")
library(gbm)
library(MASS)
str(Boston)
set.seed(1)
# train = sample(1:nrow(Boston), nrow(Boston)/2)
train = sample(nrow(Boston), 0.8*nrow(Boston))
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,
interaction.depth=4)
#  la función summary nos dá el ranking de variables y las grafica
summary(boost.boston)
str(summary(boost.boston))
summary(boost.boston)$var
View(summary(boost.boston))
plot(summary(boost.boston)$var,summary(boost.boston)$rel.inf, xlab="variable", ylab="relevancia", type = "p", col="blue" )
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
boost.boston_50=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=50,
interaction.depth=4)
#  la función summary nos dá el ranking de variables y las grafica
summary(boost.boston_50)
#  predicciones en el set de validación
y.boost=predict(boost.boston, newdata=Boston[-train,], n.trees=5000)
y.boost
mean((y.boost - Boston[-train,"medv"])^2)
boost.boston=gbm(medv~., data=Boston[train,], distribution="gaussian",
n.trees=5000, interaction.depth=4, shrinkage=0.2, verbose=F)
y.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
y.boost
mean((y.boost-Boston[-train,"medv"])^2)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",
n.trees=5000,interaction.depth=4,shrinkage=0.1,verbose=F)
y.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((y.boost-Boston[-train,"medv"])^2)
#  Vamos a probar con diferente cantidad de árboles
cant.arb =seq(1, 200, by=5)
cant.arb
matriz.pred = predict(boost.boston,newdata = Boston[-train,],n.trees = cant.arb)
matriz.pred
matriz.pred[,1:8]
matplot(cant.arb,cme,  pch=19, col="red")
cme = apply((matriz.pred - Boston[-train,"medv"])^2,2,FUN=mean)
matplot(cant.arb, cme,  pch=19, col="red")
cme = apply((matriz.pred - Boston[-train,"medv"])^2,2,FUN=mean)
matplot(cant.arb, cme,  pch=19, col="red")
names(cme[which.min(cme)])
#  GBM = GRADIENT BOOSTED MACHINES
# install.packages("gbm")
library(gbm)
ds = read.csv("C:/Users/Battaglia/Desktop/Cursos/DataMining FIUBA/Clases/06 - Árboles de Decisión/Churn_Modelling.csv")
# quito campos que no uso  factorizo la variable objetivo
ds = ds[-c(1:3)]
str(ds)
set.seed(1)
train = sample(nrow(ds), 0.8*nrow(ds))
ds
str(ds)
set.seed(1)
train = sample(nrow(ds), 0.8*nrow(ds))
boost=gbm(Exited~., data=ds[train,], distribution="adaboost", n.trees=2000)
#  predicciones en el set de validación
pred.boost.hx=predict(boost,newdata = ds[-train,],n.trees=2000)
pred.boost.hx
pred.boost.prob=predict(boost,newdata = ds[-train,],n.trees=2000, type = "response")
pred.boost.prob
head(pred.boost.hx)
head(pred.boost.prob)
pred.clase.hx <- ifelse(pred.boost.hx > 0, 1, 0)
pred.clase.prob <- ifelse(pred.boost.prob > 0.5, 1, 0)
pred.clase.prob
mc = table(pred.clase.hx,ds[-train,"Exited"])
mc
VP=mc[2,2];
VN=mc[1,1];
FP=mc[2,1];
FN=mc[1,2];
p_VP=VP/(VP+FN)
p_VN=VN/(VN+FP)
p_FP=FP/(VN+FP)
p_FN=FN/(VP+FN)
acc = (VP+VN)/(VP+VN+FP+FN)
p_FN
acc
#  Vamos a probar con diferente cantida de árboles
cant.arb =seq(100, 10000, by=10 )
cant.arb
boost=gbm(Exited~.,data=ds[train,],distribution="adaboost",n.trees=10000,
interaction.depth=3)
matriz.pred = predict(boost, newdata = ds[-train,], n.trees = cant.arb, type="response")
matriz.pred.clase <- ifelse(matriz.pred>0.5,1,0)
acc.vector = double(ncol(matriz.pred))
matriz.pred.clase
acc.vector
for (i in 1:ncol(matriz.pred)){
acc.vector[i] = sum(diag(table(matriz.pred.clase[,i],ds[-train,"Exited"])))/nrow(matriz.pred)
}
matplot(cant.arb,acc.vector,  pch=19, col="red")
boost=gbm(Exited~., data=ds[train,], distribution="adaboost", n.trees=100000,
interaction.depth=2)
cant.arb =seq(10000, 100000, by=1000)
matriz.pred = predict(boost,newdata = ds[-train,],n.trees = cant.arb, type="response")
matriz.pred = predict(boost, newdata = ds[-train,], n.trees = cant.arb, type="response")
matriz.pred.clase <- ifelse(matriz.pred>0.5,1,0)
acc.vector = double(ncol(matriz.pred))
err.vector = double(ncol(matriz.pred))
for (i in 1:ncol(matriz.pred)){
err.vector[i] = 1-sum(diag(table(matriz.pred.clase[,i],ds[-train,"Exited"])))/nrow(matriz.pred)
}
for (i in 1:ncol(matriz.pred)){
acc.vector[i] = sum(diag(table(matriz.pred.clase[,i],ds[-train,"Exited"])))/nrow(matriz.pred)
}
min(acc.vector)
cant.arb[which(acc.vector==min(acc.vector))]
matplot(cant.arb,acc.vector,  pch=10, col="red", xlab="Cant de árboles", ylab="Error")
matplot(cant.arb, acc.vector,  pch=10, col="red", xlab="Cant de árboles", ylab="Accuracy")
#  GBM = GRADIENT BOOSTED MACHINES
# install.packages("gbm")
library(gbm)
ds = read.csv("C:/Users/Battaglia/Desktop/Cursos/DataMining FIUBA/Clases/06 - Árboles de Decisión/Churn_Modelling.csv")
# quito campos que no uso  factorizo la variable objetivo
ds = ds[-c(1:3)]
str(ds)
set.seed(1)
train = sample(nrow(ds), 0.8*nrow(ds))
boost=gbm(Exited~., data=ds[train,], distribution="adaboost", n.trees=2000)
#  predicciones en el set de validación
pred.boost.hx=predict(boost,newdata = ds[-train,],n.trees=2000)
pred.boost.prob=predict(boost,newdata = ds[-train,],n.trees=2000, type = "response")
head(pred.boost.hx)
head(pred.boost.prob)
pred.clase.hx <- ifelse(pred.boost.hx > 0, 1, 0)
pred.clase.prob <- ifelse(pred.boost.prob > 0.5, 1, 0)
mc = table(pred.clase.hx,ds[-train,"Exited"])
mc
acc = (VP+VN)/(VP+VN+FP+FN)
VP=mc[2,2];
VN=mc[1,1];
FP=mc[2,1];
FN=mc[1,2];
p_VP=VP/(VP+FN)
p_VN=VN/(VN+FP)
p_FP=FP/(VN+FP)
p_FN=FN/(VP+FN)
acc = (VP+VN)/(VP+VN+FP+FN)
p_FN
acc
umbrales=(1:10)/10
umbrales
umbrales=(1:10)/10
for (i in umbrales) {
prediccion.segun.umbral <- ifelse(pred.boost.prob > i, 1, 0)
mc = table(prediccion.segun.umbral, ds[-train,"Exited"])
false.positives[i]=FP=mc[2,1]
true.positives[i]=  VP=mc[2,2]
}
umbrales=(1:10)/10
false.positives=1:10
true.positives=1:10
for (i in umbrales) {
prediccion.segun.umbral <- ifelse(pred.boost.prob > i, 1, 0)
mc = table(prediccion.segun.umbral, ds[-train,"Exited"])
false.positives[i]=FP=mc[2,1]
true.positives[i]=  VP=mc[2,2]
}
for (i in umbrales) {
prediccion.segun.umbral <- ifelse(pred.boost.prob > i, 1, 0)
mc = table(prediccion.segun.umbral, ds[-train,"Exited"])
mc
false.positives[i]=FP=mc[2,1]
true.positives[i]=  VP=mc[2,2]
}
for (i in umbrales) {
prediccion.segun.umbral <- ifelse(pred.boost.prob > i, 1, 0)
prediccion.segun.umbral
mc = table(prediccion.segun.umbral, ds[-train,"Exited"])
mc
false.positives[i]=FP=mc[2,1]
true.positives[i]=  VP=mc[2,2]
}
for (i in umbrales) {
prediccion.segun.umbral <- ifelse(pred.boost.prob > i, 1, 0)
prediccion.segun.umbral
mc = table(prediccion.segun.umbral, ds[-train,"Exited"])
mc
false.positives[i]=FP=mc[2,1]
true.positives[i]=  VP=mc[2,2]
}
for (i in umbrales) {
prediccion.segun.umbral <- ifelse(pred.boost.prob > i, 1, 0)
mc = table(prediccion.segun.umbral, ds[-train,"Exited"])
mc
false.positives[i]= mc[2,1]
true.positives[i]= mc[2,2]
}
for (i in umbrales) {
prediccion.segun.umbral <- ifelse(pred.boost.prob > i, 1, 0)
mc = table(prediccion.segun.umbral, ds[-train,"Exited"])
mc
#  false.positives[i]= mc[2,1]
# true.positives[i]= mc[2,2]
}
for (i in umbrales) {
prediccion.segun.umbral <- ifelse(pred.boost.prob > i, 1, 0)
mc = table(prediccion.segun.umbral, ds[-train,"Exited"])
print(mc)
#  false.positives[i]= mc[2,1]
# true.positives[i]= mc[2,2]
}
umbrales=(1:9)/10
false.positives=1:9
true.positives=1:9
umbrales
for (i in umbrales) {
prediccion.segun.umbral <- ifelse(pred.boost.prob > i, 1, 0)
mc = table(prediccion.segun.umbral, ds[-train,"Exited"])
print(mc)
false.positives[i]= mc[2,1]
true.positives[i]= mc[2,2]
}
plot(false.positives, true.positives)
umbrales=(1:90)/100
umbrales
false.positives=1:90
true.positives=1:90
for (i in umbrales) {
prediccion.segun.umbral <- ifelse(pred.boost.prob > i, 1, 0)
mc = table(prediccion.segun.umbral, ds[-train,"Exited"])
print(mc)
false.positives[i]= mc[2,1]
true.positives[i]= mc[2,2]
}
plot(false.positives, true.positives)
for (i in umbrales) {
prediccion.segun.umbral <- ifelse(pred.boost.prob > i, 1, 0)
mc = table(prediccion.segun.umbral, ds[-train,"Exited"])
print(mc)
false.positives[i]= mc[2,1]/nrow(ds)
true.positives[i]= mc[2,2]/nrow(ds)
}
plot(false.positives, true.positives)
plot(false.positives, true.positives)
for (i in umbrales) {
prediccion.segun.umbral <- ifelse(pred.boost.prob > i, 1, 0)
mc = table(prediccion.segun.umbral, ds[-train,"Exited"])
VP=mc[2,2];
VN=mc[1,1];
FP=mc[2,1];
FN=mc[1,2];
p_VP=VP/(VP+FN)
p_FP=FP/(VN+FP)
false.positives[i]= p_VP
true.positives[i]= p_FP
}
plot(false.positives, true.positives)
plot(false.positives, true.positives)
for (i in umbrales) {
prediccion.segun.umbral <- ifelse(pred.boost.prob > i, 1, 0)
mc = table(prediccion.segun.umbral, ds[-train,"Exited"])
print(mc)
VP=mc[2,2];
VN=mc[1,1];
FP=mc[2,1];
FN=mc[1,2];
p_VP=VP/(VP+FN)
p_FP=FP/(VN+FP)
false.positives[i]= p_VP
true.positives[i]= p_FP
}
plot(umbrales, true.positives)
plot(umbrales, false.positives)
umbrales=(1:90)/100
false.positives=1:90
true.positives=1:90
for (i in umbrales) {
prediccion.segun.umbral <- ifelse(pred.boost.prob > i, 1, 0)
mc = table(prediccion.segun.umbral, ds[-train,"Exited"])
print(mc)
VP=mc[2,2];
VN=mc[1,1];
FP=mc[2,1];
FN=mc[1,2];
p_VP=VP/(VP+FN)
p_FP=FP/(VN+FP)
false.positives[i]= p_FP
true.positives[i]= p_VP
}
plot(umbrales, true.positives)
plot(umbrales, false.positives)
#Loading Data
#----------------------------------------------------
Auto=read.table("./Auto.data",
header = T,
na.strings = "?")
fix(Auto)
dim(Auto)
names(Auto)
head(Auto)
Auto=na.omit(Auto)  #omite valores Faltantes
dim(Auto)
plot(Auto$cylinders,Auto$mpg)  #Referirse a variables con signo $ --> Data$NameVariable
attach(Auto)  #Hace las variables disponibles por nombre solamente
plot(cylinders, weight)
cylinders=as.factor(cylinders)  #transforma variable numerica en factor/categoría
plot(cylinders,mpg)
hist(mpg, col=2)  #Histograma
hist(mpg, col=6, breaks = 15)  #Histograma
pairs(Auto)
pairs(~mpg+displacement+horsepower, Auto)
plot(horsepower,mpg)
identify(horsepower,mpg,name) #al hacer click en grafico identifica punto
summary(Auto)
summary(mpg)
summary(Auto)
summary(mpg)
## Regresion Lineal ##
#----------------------------------------------------------------------------------
library(MASS)
library(ISLR)
fix(Boston)
names(Boston)
dim(Boston)
?Boston
attach(Boston)  #pone variables disponibles para ser llamadas sin Boston$VariableName
##Sintaxis lm(Y~X , data= ) o attach(data) antes para no referenciar cada vez las variables
lm.fit=lm(medv~lstat, data = Boston)  #modelo solo con lstat como varX
lm.fit
summary(lm.fit)
summary(lm.fit)
names(lm.fit)
names(summary(lm.fit))
lm.fit$coefficients       #coeficientes de la regresion
confint(lm.fit)           #intervalosde confianza
predict(lm.fit,data.frame(lstat=c(5,10,15)),interval = "confidence")  #prediccion (modelo,datos,intervaloSi/No)
#Loading Data
#----------------------------------------------------
Auto=read.table("./Auto.data",
header = T,
na.strings = "?")
fix(Auto)
dim(Auto)
names(Auto)
head(Auto)
Auto=na.omit(Auto)  #omite valores Faltantes
dim(Auto)
plot(Auto$cylinders,Auto$mpg)  #Referirse a variables con signo $ --> Data$NameVariable
attach(Auto)  #Hace las variables disponibles por nombre solamente
plot(cylinders, weight)
cylinders=as.factor(cylinders)  #transforma variable numerica en factor/categoría
plot(cylinders,mpg)
hist(mpg, col=2)  #Histograma
hist(mpg, col=6, breaks = 15)  #Histograma
pairs(Auto)
pairs(~mpg+displacement+horsepower, Auto)
plot(horsepower,mpg)
identify(horsepower,mpg,name) #al hacer click en grafico identifica punto
summary(Auto)
summary(mpg)
## Regresion Lineal ##
#----------------------------------------------------------------------------------
library(MASS)
library(ISLR)
fix(Boston)
names(Boston)
dim(Boston)
?Boston
attach(Boston)  #pone variables disponibles para ser llamadas sin Boston$VariableName
##Sintaxis lm(Y~X , data= ) o attach(data) antes para no referenciar cada vez las variables
lm.fit=lm(medv~lstat, data = Boston)  #modelo solo con lstat como varX
lm.fit
summary(lm.fit)
names(lm.fit)
names(summary(lm.fit))
lm.fit$coefficients       #coeficientes de la regresion
confint(lm.fit)           #intervalosde confianza
predict(lm.fit,data.frame(lstat=c(5,10,15)),interval = "confidence")  #prediccion (modelo,datos,intervaloSi/No)
predict(lm.fit, data.frame(lstat=c(5,10,15)),interval = "prediction")
predict(lm.fit)  #si no se especifican datos, predice con el modelo "lm.fit"
plot(lstat,medv, col="blue", pch=20)
abline(lm.fit, lwd=3, col=2)   #agrega linea con a= intercep b=slope, u objeto con coef a,b como lm.fit
plot(1:20,1:20,pch=1:20)  #distintos caracteres para dibujar del pch= 1 al 20
plot(lm.fit)  #plotea residuos de la regresion vs fitted values
plot(lm.fit$fitted.values,lm.fit$residuals)           #residuos, hechos de dos maneras diferentes
lm.fit=lm(medv~lstat+age, data=Boston)
summary(lm.fit)
lm.fit=lm(medv~., data=Boston)   #Usando todas las variables con ~.
summary(lm.fit)
names(lm.fit)
names(summary(lm.fit))  #El summary contiene R^2 y sigma (el RSE)
lm.fit=lm(medv~.-age, data=Boston)  #Todas las variables excluyendo a "age"
summary(lm.fit)
lm.fit=lm(medv~lstat*age)
summary(lm.fit)
lm.fit2=lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
par(mfrow=c(1,1))
plot(lstat, medv, col="blue", pch=20)
points(lstat, lm.fit2$fitted.values, lwd=3, col=2)  #points() para superponer al anterior
lm.fit5=lm(medv~poly(lstat,5))
summary(lm.fit5)
points(lstat,lm.fit5$fitted.values)
##QUALITATIVE PREDICTORS
#----------------------------------------------------------------------------------
attach(Carseats)
fix(Carseats)
names(Carseats)
contrasts(Carseats$ShelveLoc)  #Genera dummies variables for cualitatives automaticamente
plot(ShelveLoc ,Sales)
lm.fitQual=lm(Sales~.+Income:Advertising+Price:Age, data=Carseats)
summary(lm.fitQual)
NombreFuncion=function(){
print("Esto es una funcion en ejecucion")
x=1:10
print(x)
}
NombreFuncion()
### Validation Set Approach
#----------------------------
library(ISLR)  #Auto dataSet
set.seed(1)
dim(Auto)
train=sample(392,196) #elije al azar 196 elementos del 1:932
train
